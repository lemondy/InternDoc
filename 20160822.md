##HQL 中调用 python 程序
要实现这个必须保证：
（1）在Hive的每个节点上都安装了Python执行的包；
（2）Python中使用sysstdin输入，\t 分割，print 输出；
 (3) 在 Hive 的 HQL 语句中，使用 ADD File 来加载 Python 文件；
（4）使用 TransForm 和 Using 来调用脚本实现计算

例如如下代码：读入数据，将第二，第三列数据连接起来。

```
#!/usr/bin/env python
#-*- coding:utf-8 -*-
# filename: mapper.py
import sys
for line in sys.stdin:
  try:
    line = line.strip()
    li = line.split('\t')
    newline = li[1]+li[2]
    print newline.strip()
  except Exception, err:
    continue
```
以下是运行的 Hive 代码：

```
ADD File mapper.py
SELECT TRANSFORM(*)
USING 'python mapper.py'
AS aa
FROM mytable limit 10;
```

##Python 中执行 HQL 
如下所示：

```
sql = ""  # write your sql statement
cmd = 'hive -e """' + sql.replace('"', "\'")+'"""'
try:
  p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
  while True:
    buff = p.stdout.readline()
    if buff=='' and p.poll()!=None:
      break
except Exception, re:
  traceback.print_exc()
  
```
上面的 python 代码实质上转换成了 shell 下的 hive -e 'sql语句' 的方式执行，然后把结果重定向到控制台显示。
要执行上面的代码的前置条件：（1）安装hadoop和hive，并启动完hadoop；（2）已配置好hive的环境变量，确保在shell中能正常执行hive。
